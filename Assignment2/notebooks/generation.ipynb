{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Все места, где нужно дописать код отмечены TODO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Считывание и подготовка данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Считываем текстовые данные: все файлы должны лежать в одной папке data. \n",
    "# Проверьте, что у вас все хорошо с кодировками и текст нормально считывается.\n",
    "data = \"\"\n",
    "\n",
    "for fname in os.listdir(\"data\"):\n",
    "    with open(\"data/\"+fname) as fin:\n",
    "        text = fin.read().decode('cp1251')\n",
    "        data += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\r\n",
      "\r\n",
      "\r\n",
      " Гражданский кодекс\r\n",
      " Российской Федерации\r\n",
      " Часть третья\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "26 ноября 2001 года\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      " №146-ФЗ \r\n",
      "\r\n",
      "  Принят Государственной Думой \r\n",
      "  1 ноября \n"
     ]
    }
   ],
   "source": [
    "print data[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Для дальнейшей работы нам нужно текст перевести в числовой формат.\n",
    "chars = list(set(data))\n",
    "VOCAB_SIZE = len(chars)\n",
    "\n",
    "char_to_id = { ch:id for id,ch in enumerate(chars) }\n",
    "id_to_char = { id:ch for id,ch in enumerate(chars) }\n",
    "data_ids = [char_to_id[ch] for ch in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вспомогательные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Необходимые константы\n",
    "NUM_EPOCHS = 100\n",
    "NUM_BATCHES = 1000\n",
    "BATCH_SIZE = 100\n",
    "SEQ_LEN = 20\n",
    "LEARNING_RATE = 0.01\n",
    "GRAD_CLIP = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В процессе обучения мы будем для каждого символа входной последовательности предсказывать следующий символ. Таким образом на вход сети мы будем подавать последовательности длины SEQ_LEN и получать на выходе последовательности той же длины, но со сдвигом на один символ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_random_batch(source):\n",
    "    \"\"\"Функция, которая генерирует batch из BATCH_SIZE случайных подстрок текста source. \n",
    "    Каждая подстрока должна иметь длину SEQ_LEN.\n",
    "    \n",
    "    source - массив целых чисел - номеров символов в тексте (пример - data_ids)\n",
    "    \n",
    "    Вернуть нужно кортеж (X,y), где\n",
    "    X - матрица, в которой каждая строка - подстрока длины SEQ_LEN (подается на вход сети)\n",
    "    y - матрица, в которой каждая строка - подстрока длины SEQ_LEN, (ожидается на выходе сети)\n",
    "    Таким образом, каждая строка в y должна соответсвовать строке в X со сдвигом на один символ вправо.\n",
    "    Например, если X[0]='hell', то y[0]='ello'\n",
    "    \n",
    "    Убедитесь, что вы генерируете X и y, которые правильно соответствуют друг другу.\n",
    "    Также убедитесь, что ваша функция не вылезает за край текста (самое начало или конец текста).\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO\n",
    "    \n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "е, мер по обеспечени\n",
      ", мер по обеспечению\n"
     ]
    }
   ],
   "source": [
    "a,b = generate_random_batch(data_ids)\n",
    "print ''.join(id_to_char[id] for id in a[0,:])\n",
    "print ''.join(id_to_char[id] for id in b[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В процессе тестирования мы будем предсказывать следующий символ по SEQ_LEN предыдущих. \n",
    "Генерировать очередной символ в тестовой посдедовательнсоти можно разными способами:\n",
    "1. max_sample_fn: брать символ с максимальной вероятностью\n",
    "2. proportional_sample_fn: генерировать символ пропорционально вероятности\n",
    "3. alpha_sample_fn: генерировать символ пропорционально вероятности со следующей предобраоткой: \n",
    "    logprobs/alpha, где alpha - \"жадность\" из (0,1] - чем меньше, тем ближе генерация к выбору максимума\n",
    "    после взятия экспоненты такие вероятности нужно перенормировать\n",
    "\n",
    "Для устойчивости вычислений наша сеть будет выдавать не вероятности, а их логарифмы, поэтому не забывайте в нужных местах брать от них exp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_sample_fn(logprobs):\n",
    "    return np.argmax(logprobs) \n",
    "\n",
    "def proportional_sample_fn(logprobs):\n",
    "    # TODO\n",
    "\n",
    "def alpha_sample_fn(logprobs):\n",
    "    # TODO\n",
    "\n",
    "def generate_seed():\n",
    "    \"\"\"Функция выбирает случайное начало поседовательности из data, \n",
    "    которую мы потом можем продолжать с помощью нейросети.\n",
    "    \"\"\"\n",
    "    start = np.random.randint(0,len(data)-SEQ_LEN)\n",
    "    seed_phrase = data[start:start+SEQ_LEN]\n",
    "    return seed_phrase\n",
    "\n",
    "def generate_sample(logprobs_fn,sample_fn,seed_phrase,N=100):\n",
    "    \"\"\"Функция генерирует случайный текст при помощи нейросети и печатает его\n",
    "    \n",
    "    logprobs_fn - функция, которая по входной последовательности длины SEQ_LEN \n",
    "        предсказывает логарифмы вероятностей посдледующего символа (см. функцию train)\n",
    "    sample_fn - функция, выбирающая следующий символ одним из способов, описанных выше\n",
    "    seed_phrase - начальная фраза, с которой мы начинаем генерировать\n",
    "    N - размер генерируемого текста\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO\n",
    "    \n",
    "    print(random_snippet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Технические вещи\n",
    "\n",
    "# Вспомогательная функция для запаковки результата обучения \n",
    "def pack(err, network,inp, target,train_fn, logprobs_fn):\n",
    "    return {'err':err, \n",
    "        'network':network,\n",
    "        'inp':inp, \n",
    "        'target':target,\n",
    "        'train_fn':train_fn, \n",
    "        'logprobs_fn':logprobs_fn\n",
    "           } \n",
    "\n",
    "# numerically stable log-softmax with crossentropy\n",
    "def logsoftmax(x):\n",
    "    xdev = x-x.max(1,keepdims=True)\n",
    "    lsm = xdev - T.log(T.sum(T.exp(xdev),axis=1,keepdims=True))\n",
    "    return lsm\n",
    "def lsmCE(x,y):\n",
    "    return -T.clip(x,-20,0)[T.arange(y.shape[0]), y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нейронная сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_network(input_var=None, emb_size = 40, n_hidden = 100):\n",
    "    \"\"\"Функция строит простейшую рекуррентную сеть, которая состоит из следующих слоев:\n",
    "    \n",
    "    1. Входной слой размера [BATCH_SIZE, SEQ_LEN]\n",
    "    2. Embedding для перевода кодировки символов в нормальное представление: VOCAB_SIZE -> emb_size\n",
    "    3. Рекуррентный слой c n_hidden элементов на скрытом слое\n",
    "    4. Reshape выхода рекуррентного слоя в [-1, n_hidden]\n",
    "    5. Обычный полносвязный слой n_hidden -> VOCAB_SIZE с logsoftmax в качестве нелинейности\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(data_ids, emb_size, n_hidden, show = False):\n",
    "    \"\"\"Функция обучает нейросеть по данным data_ids\n",
    "    Следует обратить внимание на следующее:\n",
    "    1. Сеть будем учить NUM_EPOCHS эпох, в каждой из которых будет NUM_BATCHES батчей\n",
    "    2. Для того, чтобы следить за процессом обучения будем считать средний loss \n",
    "        на всех батчах в эпохе и сохранять его в массив err. Также будем генерировать \n",
    "        последовательности из случайных seeds после каждой эпохи, для этого нужна будет функция logprobs_fn,\n",
    "        которая по входу х размера [1, SEQ_LEN] будет выдавать вектор логарифмов вероятностей \n",
    "        для последующего символа размера [1, VOCAB_SIZE]. \n",
    "        Например, если x='hell', то нас интересует каким будет символ после второго l. \n",
    "    3. Так как мы вместо softmax используем logsoftmax, то в качестве loss для сети нужно использовать lsmCE\n",
    "    4. Перед тем, как делать шаг по градиенту, будем ограничивать градиент по норме\n",
    "        с помощью функции lasagne.updates.total_norm_constraint с ограничением на норму GRAD_CLIP\n",
    "    \n",
    "    \"\"\"\n",
    "    err=np.zeros(NUM_EPOCHS)\n",
    "        \n",
    "    print(\"Building network ...\")\n",
    "    # TODO\n",
    "    print(\"The network has {} params\".format(lasagne.layers.count_params(network)))\n",
    "    \n",
    "    # Функции для loss, updates, train_fn и logprobs_fn\n",
    "    # В качестве метода оптимизации рекомендуется использовать adam\n",
    "    # TODO\n",
    "    \n",
    "    print(\"Training ...\")\n",
    "    for epoch in xrange(NUM_EPOCHS):\n",
    "        start_time = time.time()\n",
    "        for batch in xrange(NUM_BATCHES):\n",
    "            # TODO\n",
    "        \n",
    "        if show:\n",
    "            seed = generate_seed()\n",
    "            print \"Seed: \",seed\n",
    "            print \"Max sample:\"\n",
    "            generate_sample(logprobs_fn, max_sample_fn, seed)\n",
    "            print \"Proportional sample:\"\n",
    "            generate_sample(logprobs_fn, proportional_sample_fn, seed)\n",
    "        print(\"Epoch {} \\t loss = {:.4f} \\t time = {:.2f}s\".\n",
    "                      format(epoch, err[epoch], time.time() - start_time))\n",
    "             \n",
    "    return pack(err, network, inp, target, train_fn, logprobs_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед тем, как запускать обучение с большим числом итераций и длинными последовательностями, попробуйте запустить его на десяток итераций с последовательнсотямит по 5 символов и проверьте, что у вас генерируются какие-то вменяемые слоги. При этом достатояно использовать довольно маленькую сеть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network ...\n",
      "The network has 72982 params\n",
      "Training ...\n",
      "Epoch 0 \t loss = 1.6287 \t time = 9.22s\n",
      "Epoch 1 \t loss = 1.3190 \t time = 9.20s\n",
      "Epoch 2 \t loss = 1.2823 \t time = 9.20s\n",
      "Epoch 3 \t loss = 1.2688 \t time = 9.18s\n",
      "Epoch 4 \t loss = 1.2614 \t time = 9.20s\n",
      "Epoch 5 \t loss = 1.2556 \t time = 9.19s\n",
      "Epoch 6 \t loss = 1.2549 \t time = 9.20s\n",
      "Epoch 7 \t loss = 1.2506 \t time = 9.26s\n",
      "Epoch 8 \t loss = 1.2486 \t time = 9.26s\n",
      "Epoch 9 \t loss = 1.2522 \t time = 9.26s\n",
      "Epoch 10 \t loss = 1.2497 \t time = 9.26s\n",
      "Epoch 11 \t loss = 1.2453 \t time = 9.22s\n",
      "Epoch 12 \t loss = 1.2447 \t time = 9.23s\n",
      "Epoch 13 \t loss = 1.2475 \t time = 9.26s\n",
      "Epoch 14 \t loss = 1.2507 \t time = 9.23s\n",
      "Epoch 15 \t loss = 1.2400 \t time = 9.25s\n",
      "Epoch 16 \t loss = 1.2489 \t time = 9.23s\n",
      "Epoch 17 \t loss = 1.2654 \t time = 9.27s\n",
      "Epoch 18 \t loss = 1.2436 \t time = 9.26s\n",
      "Epoch 19 \t loss = 1.2525 \t time = 9.27s\n",
      "Epoch 20 \t loss = 1.2448 \t time = 9.26s\n",
      "Epoch 21 \t loss = 1.2487 \t time = 9.26s\n",
      "Epoch 22 \t loss = 1.2433 \t time = 9.27s\n",
      "Epoch 23 \t loss = 1.2510 \t time = 9.26s\n",
      "Epoch 24 \t loss = 1.2524 \t time = 9.26s\n",
      "Epoch 25 \t loss = 1.2518 \t time = 9.27s\n",
      "Epoch 26 \t loss = 1.2570 \t time = 9.27s\n",
      "Epoch 27 \t loss = 1.2507 \t time = 9.25s\n",
      "Epoch 28 \t loss = 1.2535 \t time = 9.24s\n",
      "Epoch 29 \t loss = 1.2489 \t time = 9.23s\n",
      "Epoch 30 \t loss = 1.2546 \t time = 9.23s\n",
      "Epoch 31 \t loss = 1.2516 \t time = 9.23s\n",
      "Epoch 32 \t loss = 1.2541 \t time = 9.25s\n",
      "Epoch 33 \t loss = 1.2678 \t time = 9.24s\n",
      "Epoch 34 \t loss = 1.3165 \t time = 9.27s\n",
      "Epoch 35 \t loss = 1.3457 \t time = 9.27s\n",
      "Epoch 36 \t loss = 1.3096 \t time = 9.26s\n",
      "Epoch 37 \t loss = 1.3243 \t time = 9.26s\n",
      "Epoch 38 \t loss = 1.3156 \t time = 9.24s\n",
      "Epoch 39 \t loss = 1.2957 \t time = 9.25s\n",
      "Epoch 40 \t loss = 1.2905 \t time = 9.27s\n",
      "Epoch 41 \t loss = 1.2791 \t time = 9.26s\n",
      "Epoch 42 \t loss = 1.2626 \t time = 9.26s\n",
      "Epoch 43 \t loss = 1.2663 \t time = 9.26s\n",
      "Epoch 44 \t loss = 1.2643 \t time = 9.26s\n",
      "Epoch 45 \t loss = 1.2661 \t time = 9.24s\n",
      "Epoch 46 \t loss = 1.2650 \t time = 9.24s\n",
      "Epoch 47 \t loss = 1.2702 \t time = 9.24s\n",
      "Epoch 48 \t loss = 1.2697 \t time = 9.27s\n",
      "Epoch 49 \t loss = 1.2628 \t time = 9.25s\n",
      "Epoch 50 \t loss = 1.2976 \t time = 9.25s\n",
      "Epoch 51 \t loss = 1.2687 \t time = 9.25s\n",
      "Epoch 52 \t loss = 1.2780 \t time = 9.26s\n",
      "Epoch 53 \t loss = 1.2736 \t time = 9.24s\n",
      "Epoch 54 \t loss = 1.2745 \t time = 9.25s\n",
      "Epoch 55 \t loss = 1.2638 \t time = 9.25s\n",
      "Epoch 56 \t loss = 1.2649 \t time = 9.24s\n",
      "Epoch 57 \t loss = 1.2666 \t time = 9.24s\n",
      "Epoch 58 \t loss = 1.2698 \t time = 9.26s\n",
      "Epoch 59 \t loss = 1.2738 \t time = 9.24s\n",
      "Epoch 60 \t loss = 1.2786 \t time = 9.24s\n",
      "Epoch 61 \t loss = 1.2815 \t time = 9.23s\n",
      "Epoch 62 \t loss = 1.2720 \t time = 9.24s\n",
      "Epoch 63 \t loss = 1.2860 \t time = 9.24s\n",
      "Epoch 64 \t loss = 1.2672 \t time = 9.26s\n",
      "Epoch 65 \t loss = 1.2730 \t time = 9.24s\n",
      "Epoch 66 \t loss = 1.2785 \t time = 9.23s\n",
      "Epoch 67 \t loss = 1.2702 \t time = 9.26s\n",
      "Epoch 68 \t loss = 1.2694 \t time = 9.24s\n",
      "Epoch 69 \t loss = 1.2871 \t time = 9.26s\n",
      "Epoch 70 \t loss = 1.2784 \t time = 9.26s\n",
      "Epoch 71 \t loss = 1.2793 \t time = 9.25s\n",
      "Epoch 72 \t loss = 1.2956 \t time = 9.25s\n",
      "Epoch 73 \t loss = 1.2728 \t time = 9.26s\n",
      "Epoch 74 \t loss = 1.2729 \t time = 9.25s\n",
      "Epoch 75 \t loss = 1.2917 \t time = 9.25s\n",
      "Epoch 76 \t loss = 1.2760 \t time = 9.27s\n",
      "Epoch 77 \t loss = 1.3270 \t time = 9.27s\n",
      "Epoch 78 \t loss = 1.2745 \t time = 9.24s\n",
      "Epoch 79 \t loss = 1.2982 \t time = 9.23s\n",
      "Epoch 80 \t loss = 1.2792 \t time = 9.22s\n",
      "Epoch 81 \t loss = 1.2827 \t time = 9.26s\n",
      "Epoch 82 \t loss = 1.3158 \t time = 9.28s\n",
      "Epoch 83 \t loss = 1.2944 \t time = 9.27s\n",
      "Epoch 84 \t loss = 1.2843 \t time = 9.24s\n",
      "Epoch 85 \t loss = 1.3361 \t time = 9.23s\n",
      "Epoch 86 \t loss = 1.3074 \t time = 9.25s\n",
      "Epoch 87 \t loss = 1.3408 \t time = 9.25s\n",
      "Epoch 88 \t loss = 1.2808 \t time = 9.26s\n",
      "Epoch 89 \t loss = 1.2991 \t time = 9.24s\n",
      "Epoch 90 \t loss = 1.2986 \t time = 9.24s\n",
      "Epoch 91 \t loss = 1.2871 \t time = 9.25s\n",
      "Epoch 92 \t loss = 1.2882 \t time = 9.26s\n",
      "Epoch 93 \t loss = 1.2932 \t time = 9.25s\n",
      "Epoch 94 \t loss = 1.3016 \t time = 9.24s\n",
      "Epoch 95 \t loss = 1.2884 \t time = 9.26s\n",
      "Epoch 96 \t loss = 1.3104 \t time = 9.25s\n",
      "Epoch 97 \t loss = 1.2894 \t time = 9.25s\n",
      "Epoch 98 \t loss = 1.2844 \t time = 9.27s\n",
      "Epoch 99 \t loss = 1.3077 \t time = 9.26s\n"
     ]
    }
   ],
   "source": [
    "model = train(data_ids, 40, 200, show = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Посмотрим что из этого вышло"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Каждый человек должен, в пределов на катериумо от принцелях, при правонарушениях, в случае имущества и таможенных органов по правонарушения право предусмотрения или производить положение принятия и провода транспортного органов при на на в товары соответствии с подрядования составления с установление которых законом или\n"
     ]
    }
   ],
   "source": [
    "seed = u\"Каждый человек должен\"\n",
    "alpha = 0.5\n",
    "sampling_fun = alpha_sample_fn\n",
    "result_length = 300\n",
    "\n",
    "generate_sample(model['logprobs_fn'],sampling_fun,seed,result_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В случае неповиновения в отношения договором при на правонарушения имущество и не принимается ее принять авторавательного на договора или иные действия с можетственности правонарушении законной установления админислицам, должностного судом срок, принятие должностного договор и таки содержащими недействия подлежащим в соо\n"
     ]
    }
   ],
   "source": [
    "seed = u\"В случае неповиновения\"\n",
    "alpha = 0.5\n",
    "sampling_fun = alpha_sample_fn\n",
    "result_length = 300\n",
    "\n",
    "generate_sample(model['logprobs_fn'],sampling_fun,seed,result_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнительные пункты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Обучение более сложной модели и контроль переобучения. Попробуйте подобрать хорошую модель RNN для данной задачи. Для этого  проанализируйте качество работы модели в зависимости от ее размеров, попробуйте использовать многослойную сеть. Также нужно проконтролировать переобучение моделей. Для этого можно выделить тестовый кусок из текста и смотреть на то, как меняется loss на нем в процессе обучения. Если на графиках видно переобучение, то стоит добавить dropout слои в модель (обычный dropout до, между и после рекуррентных слоев). При использовании дропаута на стадии предсказания для нового объекта нужно ставить флаг deterministic=True.\n",
    "2. LSTM и GRU архитектуры. Вместо обычной RNN попробуйте LSTM и GRU архитектуры и сравните получающиеся результаты для моделей нескольких разных размеров. Также сравните модели на данных с разной SEQ_LEN. \n",
    "4. Визуализация. Попробуйте провизуализировать результаты. Например, можно смотреть на то, какие буквы модель хорошо предсказывает, а в каких сильно не уверена. Это покажет что именно выучила модель лучше всего. Также можно попробовать смотреть на активации разных скрытых нейронов при прочтении текста (как у Андрея Карпатого).\n",
    "5. Более сложные данные. Попробуйте обучить модель на более структурированных данных, например коде. Используйте LSTM и GRU сети, они хорошо улавливают структуру в данных. Проанализируйте результаты: выделите нейроны, активации которых \"отвечают\" за структуру в данных. Этот пункт, пожалуй, стоит пробовать только если у вас есть нормальный GPU.\n",
    "6. Продвинутый дропаут. Запрограммировать RNN с продвинутым дропаутом из [Gal, Ghahramani, 2016]. Сравнить с обычным вариантом дропаута по нерекуррентным связям."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed',)).History will not be written to the database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Все места, где нужно дописать код отмечены TODO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Считывание и подготовка данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Считываем данные: каждый класс лежит в своем csv файле. \n",
    "male = pd.read_csv('male.csv',header = None)[0]\n",
    "female = pd.read_csv('female.csv',header = None)[0]\n",
    "\n",
    "y = np.hstack((np.zeros(len(male)),np.ones(len(female))))\n",
    "data = list(male)\n",
    "data.extend(list(female))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Для дальнейшей работы нам понадобится словарь символов + \n",
    "# мы не будем различать строчные и прописные буквы + \n",
    "# у нас все последовательности разной длины и нам нужно понимать, какова макимальная длина\n",
    "MAX_LEN = 0\n",
    "chars = set()\n",
    "for i in xrange(len(data)):\n",
    "    data[i] = data[i].lower()\n",
    "    MAX_LEN = max(MAX_LEN,len(data[i]))\n",
    "    chars = chars.union(set(data[i][:]))\n",
    "    \n",
    "chars = list(chars)\n",
    "char_to_id = { ch:id for id,ch in enumerate(chars) }\n",
    "id_to_char = { id:ch for id,ch in enumerate(chars) }\n",
    "\n",
    "VOCAB_SIZE = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Разделим выборку на трейн и тест\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data2format(data, labels):\n",
    "    \"\"\"Функция преобразует выбоку данных в формат, подходящий для подачи в нейронную сеть.\n",
    "    \n",
    "    data - список строк (пример - X_train)\n",
    "    labels - вектор меток для строк из data (пример - y_train)\n",
    "    \n",
    "    Дальше за N обозначается число строк в data\n",
    "    \n",
    "    Вернуть нужно словарь со следующими элементами:\n",
    "    x - матрица размера [N, MAX_LEN], в которой каждая строка соответствует строке в data:\n",
    "        к строке прибавляется символы начала и конца строки, \n",
    "        после чего вся строка кодируется с помощью char_to_id.\n",
    "        Недостающие элементы в конце коротких строк заполняются нулями\n",
    "    mask - бинарная матрица размера [N, MAX_LEN]:\n",
    "        единица говорит о том, что в соответствующем элементе x стоит значащий символ\n",
    "        ноль говорит о том, что соответствующий элемент x не несет информации \n",
    "        (те самые нули, которые просто дополняют строки до MAX_LEN)\n",
    "    y - вектор длины N с метками\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO\n",
    "    return {'x':x,'mask': mask,'y': np.array(y,dtype='int32')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = data2format(X_train,y_train)\n",
    "test_data = data2format(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вспомогательные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Необходимые константы\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 100\n",
    "SEQ_LEN = 20\n",
    "LEARNING_RATE = 0.01\n",
    "GRAD_CLIP = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Технические вещи\n",
    "\n",
    "# Вспомогательная функция для запаковки результата обучения \n",
    "def pack(train_err, train_acc, test_err,test_acc, network, inp, inp_mask,target,train_fn, test_fn):\n",
    "    return {'train_err':train_err, \n",
    "        'train_acc':train_acc, \n",
    "        'test_err':test_err, \n",
    "        'test_acc':test_acc, \n",
    "        'network':network,\n",
    "        'inp':inp, \n",
    "        'inp_mask':inp_mask,\n",
    "        'target':target,\n",
    "        'train_fn':train_fn, \n",
    "        'test_fn':test_fn\n",
    "           } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нейронная сеть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В простейшем случае мы будем использовать сеть, которая считывает входную последовательность, и выдает результат только в самом конце."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_network(input_var=None, input_mask=None, emb_size = 40, n_hidden = 100):\n",
    "    \"\"\"Функция строит простейшую рекуррентную сеть, которая состоит из следующих слоев:\n",
    "    \n",
    "    1. Входной слой размера [BATCH_SIZE, MAX_LEN]. \n",
    "    2. Embedding для перевода кодировки символов в нормальное представление: VOCAB_SIZE -> emb_size\n",
    "    3. Входной слой для маски размера [BATCH_SIZE, MAX_LEN]\n",
    "    4. Рекуррентный слой c n_hidden элементов на скрытом слое:\n",
    "        * этому слою кроме обычного входа нужно подавать еще и mask для правильной работы \n",
    "            с последовательностями разной длины\n",
    "        * из этого слоя нам нужно только выход в последний момент времени, \n",
    "            его можно извлечь с помощью only_return_final\n",
    "    5. Обычный полносвязный слой для бинарной классификации с sigmoid в качестве нелинейности\n",
    "    \n",
    "    Чтобы в дальнейшем мы могли запускать сеть, например, на одной последовательности, \n",
    "    для входного слоя и маски стоит прописывать shape=(None, None)\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(train_data, test_data, emb_size, n_hidden, show = False):\n",
    "    \"\"\"Функция обучает нейросеть по данным train_data + контролирует процесс по качеству на test_data\n",
    "    Следует обратить внимание на следующее:\n",
    "    1. Сеть будем учить NUM_EPOCHS эпох, в каждой из столько батчей, сколько есть в train_data\n",
    "    2. Перед каждой эпохой желательно перемешивать данные с помощью shuffle\n",
    "    3. Для того, чтобы следить за процессом обучения будем считать средний loss и \n",
    "        среднюю точность классификации на всех батчах трейна и теста и сохранять жти данные \n",
    "        в соответствующие массивы. \n",
    "    4. Перед тем, как делать шаг по градиенту, будем ограничивать градиент по норме\n",
    "        с помощью функции lasagne.updates.total_norm_constraint с ограничением на норму GRAD_CLIP\n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"Prepare data ...\")\n",
    "    train_x, train_mask, train_y = train_data['x'], train_data['mask'],train_data['y']\n",
    "    test_x, test_mask, test_y = test_data['x'], test_data['mask'],test_data['y']\n",
    "    \n",
    "    train_size = len(train_y)\n",
    "    test_size = len(test_y)\n",
    "    num_train_batches = train_size / BATCH_SIZE\n",
    "    num_test_batches = test_size / BATCH_SIZE\n",
    "    train_err=np.zeros(NUM_EPOCHS)\n",
    "    train_acc=np.zeros(NUM_EPOCHS)\n",
    "    test_err=np.zeros(NUM_EPOCHS)\n",
    "    test_acc=np.zeros(NUM_EPOCHS)\n",
    "        \n",
    "    print(\"Building network ...\")\n",
    "    # TODO\n",
    "    print(\"The network has {} params\".format(lasagne.layers.count_params(network)))\n",
    "    \n",
    "    # Функции для loss, updates, train_fn и logprobs_fn\n",
    "    # В качетсве loss стоит взять обычную бинарную cross-entropy\n",
    "    # Для более устойчивого вычисления loss стоит обрезать предсказание \n",
    "    # перед подсчетом loss: T.clip(prediction,1e-7,1-1e-7)\n",
    "    # В качестве метода оптимизации рекомендуется использовать adam\n",
    "    \n",
    "    # TODO\n",
    "    \n",
    "    print(\"Training ...\")\n",
    "    for epoch in xrange(NUM_EPOCHS):\n",
    "        start_time = time.time()\n",
    "        # TODO\n",
    "\n",
    "        print(\"Epoch {} \\t loss / accuracy test = {:.4f}, {:.4f} \\t train = {:.4f}, {:.4f} \\t time = {:.2f}s\".\n",
    "              format(epoch, test_err[epoch],test_acc[epoch], \n",
    "                     train_err[epoch],  train_acc[epoch],time.time() - start_time))\n",
    "             \n",
    "    return pack(train_err, train_acc, test_err, test_acc, network, inp, inp_mask, target, train_fn, test_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед тем, как запускать обучение большой сети на большое число эпох, проверьте, что маленькая сеть выдает вменяемые результаты: качество больше 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare data ...\n",
      "Building network ...\n",
      "The network has 174071 params\n",
      "Training ...\n",
      "Epoch 0 \t loss / accuracy test = 0.4417, 78.1739 \t train = 0.4343, 78.0545 \t time = 0.63s\n",
      "Epoch 1 \t loss / accuracy test = 0.4322, 79.0870 \t train = 0.4265, 78.4727 \t time = 0.61s\n",
      "Epoch 2 \t loss / accuracy test = 0.4198, 80.1304 \t train = 0.3976, 80.7636 \t time = 0.61s\n",
      "Epoch 3 \t loss / accuracy test = 0.4240, 79.9565 \t train = 0.4001, 80.4909 \t time = 0.61s\n",
      "Epoch 4 \t loss / accuracy test = 0.4241, 79.3043 \t train = 0.3799, 80.8909 \t time = 0.61s\n",
      "Epoch 5 \t loss / accuracy test = 0.4179, 79.7391 \t train = 0.3680, 81.4727 \t time = 0.61s\n",
      "Epoch 6 \t loss / accuracy test = 0.4188, 80.3913 \t train = 0.3610, 82.9273 \t time = 0.61s\n",
      "Epoch 7 \t loss / accuracy test = 0.4368, 79.3043 \t train = 0.3863, 81.3455 \t time = 0.61s\n",
      "Epoch 8 \t loss / accuracy test = 0.4314, 80.2174 \t train = 0.3356, 84.1091 \t time = 0.62s\n",
      "Epoch 9 \t loss / accuracy test = 0.4143, 81.1739 \t train = 0.3335, 83.7273 \t time = 0.61s\n",
      "Epoch 10 \t loss / accuracy test = 0.4365, 78.9565 \t train = 0.3530, 83.8364 \t time = 0.61s\n",
      "Epoch 11 \t loss / accuracy test = 0.4151, 79.6957 \t train = 0.3409, 84.1273 \t time = 0.61s\n",
      "Epoch 12 \t loss / accuracy test = 0.4263, 79.0435 \t train = 0.3406, 83.8364 \t time = 0.61s\n",
      "Epoch 13 \t loss / accuracy test = 0.4468, 79.3043 \t train = 0.3233, 84.7636 \t time = 0.61s\n",
      "Epoch 14 \t loss / accuracy test = 0.4208, 79.3043 \t train = 0.3387, 83.5273 \t time = 0.61s\n",
      "Epoch 15 \t loss / accuracy test = 0.4410, 77.1739 \t train = 0.3942, 79.4182 \t time = 0.61s\n",
      "Epoch 16 \t loss / accuracy test = 0.4336, 79.6957 \t train = 0.3118, 85.2909 \t time = 0.61s\n",
      "Epoch 17 \t loss / accuracy test = 0.4360, 80.4348 \t train = 0.3033, 85.5273 \t time = 0.61s\n",
      "Epoch 18 \t loss / accuracy test = 0.4412, 80.3478 \t train = 0.3055, 85.9091 \t time = 0.61s\n",
      "Epoch 19 \t loss / accuracy test = 0.4694, 78.3043 \t train = 0.3439, 83.9636 \t time = 0.61s\n"
     ]
    }
   ],
   "source": [
    "model = train(train_data, test_data, 30, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Посмотрим что из этого вышло"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(name, model):\n",
    "    \"\"\"Функция выдает предсказание обученной модели model для имени name.\n",
    "    Предсказание - число из [0,1] - вероятность того, что имя женское\n",
    "    \"\"\"\n",
    "    \n",
    "    #TODO\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = set(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a new name\n",
      "It's male name\n",
      "[ 0.04011425]\n"
     ]
    }
   ],
   "source": [
    "name = 'Yaroslav'\n",
    "if name.lower() in dataset:\n",
    "    print 'This name is in our dataset'\n",
    "else:\n",
    "    print 'This is a new name'\n",
    "pred = predict(name, model)\n",
    "if pred>=0.5:\n",
    "    print \"It's female name\"\n",
    "else:\n",
    "    print \"It's male name\"\n",
    "print pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a new name\n",
      "It's female name\n",
      "[ 0.99993134]\n"
     ]
    }
   ],
   "source": [
    "name = 'Polina'\n",
    "if name.lower() in dataset:\n",
    "    print 'This name is in our dataset'\n",
    "else:\n",
    "    print 'This is a new name'\n",
    "pred = predict(name, model)\n",
    "if pred>=0.5:\n",
    "    print \"It's female name\"\n",
    "else:\n",
    "    print \"It's male name\"\n",
    "print pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнительные пункты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Обучение более сложной модели и контроль переобучения. Попробуйте подобрать хорошую модель RNN для данной задачи. Для этого проанализируйте качество работы модели в зависимости от ее размеров, попробуйте использовать многослойную сеть. Также нужно проконтролировать переобучение моделей. Для этого можно выделить тестовый кусок из текста и смотреть на то, как меняется loss на нем в процессе обучения. Если на графиках видно переобучение, то стоит добавить dropout слои в модель (обычный dropout до, между и после рекуррентных слоев). При использовании дропаута на стадии предсказания для нового объекта нужно ставить флаг deterministic=True.\n",
    "2. Другая архитектура 1. Попробуйте использовать не только состоянию скрытых переменных в последний момент времени, а усреднение/максимум значений скрытых переменных во все моменты времени. Попробуйте двунаправленную сеть при таком подходе. \n",
    "3. Другая архитектура 2. Попробуйте использовать не только состоянию скрытых переменных в последний момент времени, а сумму значений скрытых переменных во все моменты времени с коэффициентами attention. Попробуйте двунаправленную сеть при таком подходе. Attention коэффициент для определенного момента времени может представлять собой просто линейную комбинацию значений скрытых переменных в этот момент времени с обучаемыми весами.\n",
    "3. Визуализация. Попробуйте провизуализировать результаты. Например, для стандартной архитектуры можно посмотреть на изменение предсказания во времени: на каких элементах предсказание значительнее всего изменяется в сторону одного или другого класса? При использовании схемы из 2/3 пункта, можно смотреть на вклад каждого момента времени в результат. Так как после рекуррентного слоя у нас стоит просто линейный классификатор, то можно посмотреть, что выдает этот классификатор при применении к скрытым переменным в каждый момент времени. Таким образом выделяться те буквы, которые голосуют за один класс и те, которые голосуют за другой.\n",
    "4. Batchnorm и Layernorm. Запрограммируйте RNN c layer normalization из статьи [Lei Ba et al., 2016]. Поэкспериментируйте с применением обычной batch normalization и layer normalization, сравните результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
